"""VLM API client for scene analysis and manipulation planning."""

import base64
import io
import json
import logging
from dataclasses import dataclass, field

import numpy as np

from app.core.config import settings

logger = logging.getLogger(__name__)

SCENE_ANALYSIS_PROMPT = """You are a robotic manipulation planner. You are controlling a Shadow Hand (5-finger dexterous hand) mounted on a 7-DOF arm.

Analyze the scene image and plan a manipulation action for the following task:
Task: {task_description}

Object information:
{object_info}

Current phase: {phase}

Based on the image, provide a manipulation plan as JSON with these fields:
{{
  "grasp_point": [x, y, z],       // 3D point on the object to grasp (meters, world frame)
  "approach_vector": [x, y, z],    // direction to approach from (unit vector)
  "finger_config": {{
    "thumb": 0.0-1.0,              // 0=open, 1=fully closed
    "index": 0.0-1.0,
    "middle": 0.0-1.0,
    "ring": 0.0-1.0,
    "pinky": 0.0-1.0
  }},
  "force_level": "gentle|medium|firm",
  "ee_target": [x, y, z],         // target end-effector position
  "wrist_orientation": [roll, pitch, yaw],  // desired wrist angles in radians
  "notes": "brief reasoning"
}}

Respond with ONLY the JSON object, no other text."""

COMMAND_INTERPRETATION_PROMPT = """You are controlling a humanoid robot arm with:
- A 7-DOF arm (shoulder pan/lift/roll, elbow, wrist roll/pitch/yaw)
- A 5-finger dexterous hand (thumb, index, middle, ring, pinky)
  Each finger: 0.0 = fully open, 1.0 = fully closed

The end-effector workspace (meters, world frame):
- X: forward/back (0.2 to 0.8)
- Y: left/right (-0.4 to 0.4)
- Z: height (0.1 to 0.8)
- Home position: [0.3, 0.0, 0.6]
- Wrist orientation: [roll, pitch, yaw] in radians

Scene context:
{object_info}

User command: {command}

Plan a sequence of motion steps to execute this command. Each step smoothly
moves the arm to a target pose with a finger configuration over a duration.

Return ONLY a JSON object:
{{
  "description": "Brief summary of the overall plan",
  "steps": [
    {{
      "description": "What this step accomplishes",
      "ee_target": [x, y, z],
      "wrist_orientation": [roll, pitch, yaw],
      "finger_config": {{"thumb": 0.0, "index": 0.0, "middle": 0.0, "ring": 0.0, "pinky": 0.0}},
      "duration_steps": 100
    }}
  ]
}}

Guidelines:
- Use 2-8 steps. Each step transitions smoothly from the previous one.
- duration_steps: 50 = fast, 100 = normal, 200 = slow/careful
- For grasping: approach above object → descend to it → close fingers → lift
- For pointing: move arm toward target, extend only index finger (others closed)
- For waving: raise arm high, add oscillating wrist roll steps
- For pushing: approach with closed fist, move through target
- For touching/poking: extend index, move to contact point
- Be creative for unusual commands — map intent to arm+hand motions
- Keep positions within workspace bounds"""

SUCCESS_EVAL_PROMPT = """You are evaluating the result of a robotic manipulation task.

Task: {task_description}
Initial object position: {initial_pos}
Final object position: {final_pos}

Look at the final scene image and evaluate:
{{
  "success": true/false,
  "confidence": 0.0-1.0,
  "reasoning": "brief explanation",
  "grasp_quality": "poor|fair|good|excellent",
  "stability": "unstable|marginal|stable|very_stable"
}}

Respond with ONLY the JSON object."""


@dataclass
class ManipulationPlan:
    """Structured output from VLM scene analysis."""
    grasp_point: list[float] = field(default_factory=lambda: [0.5, 0.0, 0.3])
    approach_vector: list[float] = field(default_factory=lambda: [0.0, 0.0, -1.0])
    finger_config: dict[str, float] = field(
        default_factory=lambda: {"thumb": 0.5, "index": 0.5, "middle": 0.5,
                                  "ring": 0.5, "pinky": 0.5}
    )
    force_level: str = "medium"
    ee_target: list[float] = field(default_factory=lambda: [0.5, 0.0, 0.4])
    wrist_orientation: list[float] = field(default_factory=lambda: [0.0, -0.3, 0.0])
    notes: str = ""


@dataclass
class MotionStep:
    """A single waypoint in a motion plan."""
    description: str = ""
    ee_target: list[float] = field(default_factory=lambda: [0.3, 0.0, 0.5])
    wrist_orientation: list[float] = field(default_factory=lambda: [0.0, -0.3, 0.0])
    finger_config: dict[str, float] = field(
        default_factory=lambda: {"thumb": 0.0, "index": 0.0, "middle": 0.0,
                                  "ring": 0.0, "pinky": 0.0}
    )
    duration_steps: int = 100


@dataclass
class MotionPlan:
    """A sequence of motion steps generated by the VLM for any command."""
    description: str = ""
    steps: list[MotionStep] = field(default_factory=list)


def _encode_image(image: np.ndarray) -> str:
    """Encode numpy image array to base64 PNG string."""
    from PIL import Image

    if image.dtype != np.uint8:
        image = (np.clip(image, 0, 1) * 255).astype(np.uint8)

    pil_img = Image.fromarray(image)
    buf = io.BytesIO()
    pil_img.save(buf, format="PNG")
    buf.seek(0)
    return base64.standard_b64encode(buf.read()).decode("utf-8")


def _call_anthropic(image_b64: str, prompt: str) -> str:
    """Call Anthropic API with image and text prompt."""
    import anthropic

    client = anthropic.Anthropic(api_key=settings.VLM_API_KEY)
    message = client.messages.create(
        model=settings.VLM_MODEL,
        max_tokens=1024,
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "image",
                        "source": {
                            "type": "base64",
                            "media_type": "image/png",
                            "data": image_b64,
                        },
                    },
                    {"type": "text", "text": prompt},
                ],
            }
        ],
    )
    return message.content[0].text


def _call_openai(image_b64: str, prompt: str) -> str:
    """Call OpenAI API with image and text prompt."""
    import openai

    client = openai.OpenAI(api_key=settings.VLM_API_KEY)
    response = client.chat.completions.create(
        model=settings.VLM_MODEL,
        max_tokens=1024,
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/png;base64,{image_b64}",
                        },
                    },
                    {"type": "text", "text": prompt},
                ],
            }
        ],
    )
    return response.choices[0].message.content


def _call_vlm(image_b64: str, prompt: str) -> str:
    """Route to the configured VLM provider."""
    provider = settings.VLM_PROVIDER.lower()
    if provider == "anthropic":
        return _call_anthropic(image_b64, prompt)
    elif provider == "openai":
        return _call_openai(image_b64, prompt)
    else:
        raise ValueError(f"Unknown VLM provider: {provider}")


def _parse_json_response(text: str) -> dict:
    """Extract JSON from VLM response, handling markdown code fences."""
    text = text.strip()
    if text.startswith("```"):
        # Strip code fences
        lines = text.split("\n")
        lines = [l for l in lines if not l.strip().startswith("```")]
        text = "\n".join(lines)
    return json.loads(text)


def analyze_scene(
    image: np.ndarray,
    task_description: str,
    object_info: dict,
    phase: str = "initial",
) -> ManipulationPlan:
    """Analyze a scene image and return a manipulation plan.

    Args:
        image: RGB image from camera (H, W, 3) uint8
        task_description: e.g. "grasp the object and lift it 20cm"
        object_info: dict with mass, dimensions, material, etc.
        phase: current manipulation phase for context

    Returns:
        ManipulationPlan with grasp strategy
    """
    image_b64 = _encode_image(image)
    prompt = SCENE_ANALYSIS_PROMPT.format(
        task_description=task_description,
        object_info=json.dumps(object_info, indent=2),
        phase=phase,
    )

    try:
        response_text = _call_vlm(image_b64, prompt)
        data = _parse_json_response(response_text)
        return ManipulationPlan(
            grasp_point=data.get("grasp_point", [0.5, 0.0, 0.3]),
            approach_vector=data.get("approach_vector", [0.0, 0.0, -1.0]),
            finger_config=data.get("finger_config",
                                    {"thumb": 0.5, "index": 0.5, "middle": 0.5,
                                     "ring": 0.5, "pinky": 0.5}),
            force_level=data.get("force_level", "medium"),
            ee_target=data.get("ee_target", [0.5, 0.0, 0.4]),
            wrist_orientation=data.get("wrist_orientation", [0.0, -0.3, 0.0]),
            notes=data.get("notes", ""),
        )
    except Exception as e:
        logger.warning("VLM scene analysis failed: %s; using default plan", e)
        return ManipulationPlan()


def evaluate_success_vlm(
    image: np.ndarray,
    task_description: str,
    initial_pos: list[float],
    final_pos: list[float],
) -> dict:
    """Use VLM to evaluate manipulation success from final scene image."""
    image_b64 = _encode_image(image)
    prompt = SUCCESS_EVAL_PROMPT.format(
        task_description=task_description,
        initial_pos=initial_pos,
        final_pos=final_pos,
    )

    try:
        response_text = _call_vlm(image_b64, prompt)
        return _parse_json_response(response_text)
    except Exception as e:
        logger.warning("VLM success evaluation failed: %s", e)
        return {"success": False, "confidence": 0.0, "reasoning": f"VLM error: {e}"}


def interpret_command(
    image: np.ndarray,
    command: str,
    object_info: dict | None = None,
) -> MotionPlan:
    """Interpret a free-form natural language command into a MotionPlan.

    The VLM generates a sequence of waypoints that the robot follows,
    enabling arbitrary tasks (not just grasping).

    Args:
        image: RGB image from camera (H, W, 3) uint8
        command: Free-form text, e.g. "pick up the object", "wave hello"
        object_info: Optional dict with object/scene metadata

    Returns:
        MotionPlan with a list of MotionSteps
    """
    image_b64 = _encode_image(image)
    prompt = COMMAND_INTERPRETATION_PROMPT.format(
        command=command,
        object_info=json.dumps(object_info or {}, indent=2),
    )

    _default_fingers = {"thumb": 0.0, "index": 0.0, "middle": 0.0,
                        "ring": 0.0, "pinky": 0.0}

    try:
        response_text = _call_vlm(image_b64, prompt)
        data = _parse_json_response(response_text)

        steps = []
        for s in data.get("steps", []):
            steps.append(MotionStep(
                description=s.get("description", ""),
                ee_target=s.get("ee_target", [0.3, 0.0, 0.5]),
                wrist_orientation=s.get("wrist_orientation", [0.0, -0.3, 0.0]),
                finger_config=s.get("finger_config", _default_fingers),
                duration_steps=max(int(s.get("duration_steps", 100)), 20),
            ))

        if not steps:
            raise ValueError("VLM returned empty steps list")

        return MotionPlan(
            description=data.get("description", ""),
            steps=steps,
        )
    except Exception as e:
        logger.warning("VLM command interpretation failed: %s; using default plan", e)
        # Fallback: simple reach-forward motion so the robot visibly does something
        return MotionPlan(
            description=f"Fallback plan for: {command}",
            steps=[
                MotionStep(
                    description="Reach forward",
                    ee_target=[0.5, 0.0, 0.4],
                    duration_steps=120,
                ),
                MotionStep(
                    description="Return home",
                    ee_target=[0.3, 0.0, 0.6],
                    duration_steps=120,
                ),
            ],
        )
